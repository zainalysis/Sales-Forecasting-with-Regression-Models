# -*- coding: utf-8 -*-
"""Sales_Prediction_using_gb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qE5uSuE8ggOKTJOMW9fC5QyjltsSVLJ8
"""

from google.colab import drive

drive.mount('/content/drive')

file_path='/content/drive/My Drive/sales_prediction.csv'

"""## Dataset Details

![](https://i.imgur.com/WlgNuFs.png)
"""

import pandas as pd

df = pd.read_csv(file_path)
df.head()

"""#Preparing Trainging & Test Data"""

X = df.drop(columns= ['Item_Outlet_Sales'])
y = df['Item_Outlet_Sales']

SEED=42

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)

X_train.shape, X_test.shape

X_train.head(5)

y_train.head(5)

"""#Data Transformation & Analysis"""

#copying data
X_train_c = X_train.copy()
X_train_c

"""#EDA"""

X_train_c.info()

X_train_c.isnull()

X_train_c.isnull().sum()

num_data = X_train_c.select_dtypes(exclude = ['object'])
num_data.head(10)

#checking describe to see the basic stats about the dataframe
num_data.describe()

num_data.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

fig, ax = plt.subplots (1,2, figsize=(12,5))

 sns.histplot(data=X_train_c, x='Item_Weight', ax=ax[0])
 sns.boxplot(data=X_train_c, x='Item_Weight', ax=ax[1])

 plt.show()

import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Create subplots with 1 row and 2 columns
fig = make_subplots(rows=1, cols=2, subplot_titles=('Item Weight Distribution (Histogram)', 'Item Weight Distribution (Boxplot)'))

# Histogram in the first subplot
fig.add_trace(
    go.Histogram(x=X_train_c['Item_Weight'], name='Histogram'),
    row=1, col=1
)

# Boxplot in the second subplot
fig.add_trace(
    go.Box(x=X_train_c['Item_Weight'], name='Boxplot'),
    row=1, col=2
)

# Update layout
fig.update_layout(title_text='Item Weight Distribution: Histogram and Boxplot', width=900, height=400)

# Show the plot
fig.show()

def visualize_numeric_feature(data_frame, col_name):
  fig, ax = plt.subplots(1,2, figsize=(12,5))

  sns.histplot(data=data_frame, x=col_name, ax=ax[0]);
  sns.boxplot(data=data_frame, y=col_name, ax=ax[1]);

visualize_numeric_feature(X_train_c, 'Item_Weight')

visualize_numeric_feature(X_train_c, 'Item_Visibility')

visualize_numeric_feature(X_train_c, 'Item_MRP')

visualize_numeric_feature(X_train_c, 'Outlet_Establishment_Year')

sns.countplot(data=X_train_c, x='Outlet_Establishment_Year')

#chosing datatype which has attribute of object
cat_features = X_train_c.select_dtypes(include=['object'])
cat_features.head()

#for some basic stats
cat_features.describe()

#to check the null values
cat_features.isnull().sum()

cat_features['Item_Identifier'].value_counts()

cat_features['Item_Fat_Content'].value_counts()

cat_features['Item_Type'].value_counts()

cat_features['Outlet_Identifier'].value_counts()

cat_features['Outlet_Size'].value_counts()

cat_features['Outlet_Location_Type'].value_counts()

cat_features['Outlet_Type'].value_counts()

"""#Data Wrangling + Feature Engineering

Create High Level Item Types
"""

X_train_c['Item_Identifier']

X_train_c['Item_Identifier'].apply(lambda x: x[:2])

X_train_c['Item_Identifier'].apply(lambda x: x[:2]).value_counts()

X_train_c['Item_Identifier'].str[:2]

X_train_c['Item_Identifier'].str[:2].value_counts()

"""# Mapping the Item ID to Item Types"""

def create_item_type(data_frame):
  data_frame['Item_Type']= data_frame['Item_Identifier'].str[0:2]
  data_frame['Item_Type']= data_frame['Item_Type'].str[0:2].map({
      'FD': 'Food',
      'NC': 'Non-Consumable',
      'DR': 'Drinks'
  })
  return data_frame
  data_frame.head()

X_train_c = create_item_type(X_train_c)
X_train_c.head()

"""#Filling in the missing values"""

X_train_c.isnull().sum()

X_train_c[['Item_Identifier', 'Item_Weight']].drop_duplicates().sort_values(by=['Item_Identifier'])

"""#Fill in missing values for Item_Weight"""

# Step 1: Fill missing Item_Weight using Item ID mapping
# Step 2: If Item ID is new, fill missing weight with median by item type


# Create a pivot table to map Item Identifier to Item Weight
ITEM_ID_WEIGHT_PIVOT = X_train_c.pivot_table(values='Item_Weight', index='Item_Identifier').reset_index()

# Create a dictionary mapping Item Identifiers to Item Weights using the pivot table
ITEM_ID_WEIGHT_MAPPING = dict(zip(ITEM_ID_WEIGHT_PIVOT['Item_Identifier'], ITEM_ID_WEIGHT_PIVOT['Item_Weight']))

# Display the first 10 mappings from the dictionary for verification
list(ITEM_ID_WEIGHT_MAPPING.items())[:10]

# Create a pivot table for mapping
ITEM_TYPE_WEIGHT_PIVOT = X_train_c.pivot_table(values='Item_Weight', index='Item_Type', aggfunc='median').reset_index()

# Create a dictionary mapping Item Type to median
ITEM_TYPE_WEIGHT_MAPPING = dict(zip(ITEM_TYPE_WEIGHT_PIVOT['Item_Type'], ITEM_TYPE_WEIGHT_PIVOT['Item_Weight']))

# Display the mapping of Item Type to median Item Weight
ITEM_TYPE_WEIGHT_MAPPING.items()

def impute_item_weight(data_frame):
    # Fill missing weights by Item Identifier
    data_frame.loc[:, 'Item_Weight'] = data_frame.loc[:, 'Item_Identifier'].map(ITEM_ID_WEIGHT_MAPPING).fillna(data_frame.loc[:, 'Item_Weight'])

    # Fill remaining missing weights by Item Type
    data_frame.loc[:, 'Item_Weight'] = data_frame.loc[:, 'Item_Type'].map(ITEM_TYPE_WEIGHT_MAPPING).fillna(data_frame.loc[:, 'Item_Weight'])

    return data_frame

X_train_c = impute_item_weight (X_train_c)

X_train_c.isnull().sum()

X_train_c.groupby(by=['Outlet_Type' , 'Outlet_Size']).size()

X_train_c.pivot_table(values='Outlet_Size', index='Outlet_Type', aggfunc=lambda x: x.mode()[0]).reset_index()

# Fill missing Outlet_Size based on Outlet_Type
OUTLET_TYPE_SIZE_PIVOT = X_train_c.pivot_table(values='Outlet_Size', index='Outlet_Type',
                                               aggfunc=lambda x: x.mode()[0]).reset_index()

# Create mapping for Outlet_Type and Outlet_Size
OUTLET_TYPE_SIZE_MAPPING = dict(zip(OUTLET_TYPE_SIZE_PIVOT['Outlet_Type'], OUTLET_TYPE_SIZE_PIVOT['Outlet_Size']))

# Access the mapping
OUTLET_TYPE_SIZE_MAPPING.items()

def impute_outlet_size(data_frame):
    # Fill missing Outlet_Size based on Outlet_Type mapping
    data_frame.loc[:, 'Outlet_Size'] = data_frame.loc[:, 'Outlet_Size'].fillna(data_frame.loc[:, 'Outlet_Type'].map(OUTLET_TYPE_SIZE_MAPPING))

    return data_frame

X_train_c = impute_outlet_size(X_train_c)

X_train_c.isnull().sum()

"""
#Standardize Item Fat Content Category"""

X_train_c['Item_Fat_Content'].value_counts()

def standardize_item_fat_content(data_frame):
  data_frame['Item_Fat_Content'] = data_frame['Item_Fat_Content'].replace({
      'LF': 'Low_Fat',
      'low fat': 'Low_Fat',
      'Low Fat': 'Low_Fat',
      'reg': 'Regular'
  })
  return data_frame

X_train_c = standardize_item_fat_content(X_train_c)
X_train_c['Item_Fat_Content'].value_counts()

"""#Correct Iteam Fat Content For Non Consumables"""

X_train_c.groupby(by=['Item_Type' , 'Item_Fat_Content']).size()

X_train_c.loc[X_train_c['Item_Type'] == 'Non-Consumable', 'Item_Fat_Content']

def correct_item_fat_content (data_frame):
  data_frame.loc[data_frame['Item_Type'] == 'Non-Consumable', 'Item_Fat_Content'] = 'Non-Edible'
  return data_frame

X_train_c = correct_item_fat_content (X_train_c)
X_train_c.groupby(by=['Item_Type' , 'Item_Fat_Content']).size()

X_train_c.head()

X_train_c.info()

"""#Prepration of Data Set for Machine Learning"""

def prepare_dataset(data_frame):
  data_frame = create_item_type(data_frame)
  data_frame = impute_item_weight(data_frame)
  data_frame = impute_outlet_size(data_frame)
  data_frame = standardize_item_fat_content(data_frame)
  data_frame = correct_item_fat_content(data_frame)
  return data_frame

X_train.isnull().sum()

X_train = prepare_dataset(X_train)
X_train.isnull().sum()

X_test.isnull().sum()

X_test = prepare_dataset(X_test)
X_test.isnull().sum()

"""#Handling Categorical Data

**All Categorical Columns **
"""

cat_feats = X_train.select_dtypes(include=['object'])
cat_feats.head()

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(handle_unknown='ignore')
ohe.fit(cat_feats)

ohe_feature_names = ohe.get_feature_names_out(input_features=cat_feats.columns)
ohe_feature_names

num_feats_train = X_train.select_dtypes(exclude=['object']).reset_index(drop=True)
num_feats_train.head()

cat_feats_train = X_train.select_dtypes(include=['object'])
X_train_cat_one = pd.DataFrame(ohe.transform(cat_feats_train).toarray(), columns=ohe_feature_names)
X_train_cat_one.head()

X_train_final = pd.concat([num_feats_train, X_train_cat_one], axis=1)
X_train_final.head()

final_columns = X_train_final.columns.values
final_columns

num_feats_test = X_test.select_dtypes(exclude=['object']).reset_index(drop=True)
cat_feats_test = X_test.select_dtypes(include=['object'])
X_test_cat_ohe = pd.DataFrame(ohe.transform(cat_feats_test).toarray(), columns=ohe_feature_names)
X_test_final = pd.concat([num_feats_test, X_test_cat_ohe], axis = 1)
X_test_final = X_test_final[final_columns]

X_test_final.head()

"""#Start Modeling

#1
"""

sns.histplot(y_train)

import plotly.express as px

# Assuming y_train is a list or numpy array
fig = px.histogram(y_train, nbins=30)  # You can adjust nbins based on your preference
fig.show()

pip install dask

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor
import xgboost as xgb
from lightgbm import LGBMRegressor
from sklearn.model_selection import cross_validate
import numpy as np

from sklearn.model_selection import cross_validate
import numpy as np

def train_and_eval_model(model, X_train, y_train, cv=5):
    # Perform cross-validation with R^2 and RMSE as metrics
    cv_results = cross_validate(model, X_train, y_train, cv=cv, scoring=('r2', 'neg_root_mean_squared_error'))

    # R^2 scores
    print('Model:', model)
    r2_scores = cv_results['test_r2']
    print('R2 CV scores:', r2_scores)
    print('R2 CV scores mean / stdev:', np.mean(r2_scores), '/', np.std(r2_scores))

    # RMSE scores
    rmse_scores = cv_results['test_neg_root_mean_squared_error']
    rmse_scores = [-1 * score for score in rmse_scores]  # Convert negative RMSE to positive values
    print('RMSE CV scores:', rmse_scores)
    print('RMSE CV scores mean / stdev:', np.mean(rmse_scores), '/', np.std(rmse_scores))

rf = RandomForestRegressor(random_state=SEED)
train_and_eval_model(model=rf, X_train = X_train_final, y_train=y_train)

gb = GradientBoostingRegressor(random_state=SEED)
train_and_eval_model(model=gb, X_train = X_train_final, y_train=y_train)

hgb = HistGradientBoostingRegressor(random_state=SEED)
train_and_eval_model(model=hgb, X_train = X_train_final, y_train=y_train)

xgr = xgb.XGBRegressor(objective='reg:squarederror', random_state=SEED)
train_and_eval_model(model=xgr, X_train = X_train_final, y_train=y_train)

lgbr = LGBMRegressor(random_state=SEED)
train_and_eval_model(model=lgbr, X_train = X_train_final, y_train=y_train)

"""#Handling Categorical Data

2- All Categorical Columns - Native Handling
"""

X_train_copy = X_train.copy().drop(columns='Item_Identifier')

cat_cols = X_train_copy.select_dtypes(include = ['object']).columns.tolist()
num_cols = cal_cols = X_train_copy.select_dtypes(exclude = ['object']).columns.tolist()

cat_cols, num_cols

X_train_copy[cat_cols] = X_train_copy[cat_cols].astype('category')
n_categorical_features = len(cat_cols)
n_numerical_features = len(num_cols)
X_train_copy = X_train_copy [ cat_cols+num_cols]

X_train_copy.info()

categorical_mask = [True] * n_categorical_features + [False] * n_numerical_features
categorical_mask

from sklearn.preprocessing import OrdinalEncoder
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer
from sklearn.compose import make_column_selector

categorical_mask = [True] * n_categorical_features + [False] * n_numerical_features

ordinal_encoder = make_column_transformer(
    (
        OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=np.nan),
        make_column_selector(dtype_include="category")
    ),
    remainder="passthrough",
)

hgb = make_pipeline(
    ordinal_encoder,
    HistGradientBoostingRegressor(
        random_state=42, categorical_features=categorical_mask
    ),
)
train_and_eval_model(model=hgb, X_train = X_train_copy, y_train=y_train)

lgbr = LGBMRegressor(random_state=SEED)
train_and_eval_model(model=lgbr, X_train = X_train_copy, y_train=y_train)

"""#Hadeling Categorical Data

3- No Iteam Identifier - One Hot Encoded
"""

cat_feats = X_train.select_dtypes(include=['object']).drop(columns=['Item_Identifier'])
ohe = OneHotEncoder ( handle_unknown = 'ignore')
ohe.fit(cat_feats)
ohe_feature_names = ohe.get_feature_names_out(input_features=cat_feats.columns)

num_feats_train = X_train.select_dtypes(exclude=['object']).reset_index(drop=True)
cat_feats_train = X_train.select_dtypes(include=['object']).drop(columns=['Item_Identifier'])
X_train_cat_ohe = pd.DataFrame(ohe.transform(cat_feats_train).toarray(), columns=ohe_feature_names)
X_train_final = pd.concat([num_feats_train, X_train_cat_ohe], axis =1)
X_train_final

X_train_final.shape

gb = GradientBoostingRegressor(random_state=SEED)
train_and_eval_model(model=gb, X_train=X_train_final, y_train=y_train)

hgb = HistGradientBoostingRegressor(random_state=SEED)
train_and_eval_model(model=hgb, X_train=X_train_final, y_train=y_train)

xgr = xgb.XGBRegressor(objective='reg:squarederror', random_state=SEED)
train_and_eval_model(model=xgr, X_train=X_train_final, y_train=y_train)

lgbr = LGBMRegressor(random_state=SEED)
train_and_eval_model(model=lgbr, X_train=X_train_final, y_train=y_train)

X_train.select_dtypes(include=['object']).drop(columns=['Item_Identifier']).head()

"""#Handling Categorical Data

4- Item Identifier = feature hashed, rest categorical - one hot encoded
"""

X_train['Item_Identifier'].head()

from sklearn.feature_extraction import FeatureHasher
import pandas as pd

# Specify the hash vector size
hash_vector_size = 50

# Create a FeatureHasher
fh = FeatureHasher(n_features=hash_vector_size, input_type='string')

# Ensure each entry is an iterable (e.g., list of characters or words)
# Here, we split each identifier into a list of characters
X_transformed = X_train['Item_Identifier'].apply(list)

# Transform the tokenized data with FeatureHasher
hashed_df = pd.DataFrame(fh.transform(X_transformed).toarray(),
                         columns=['H' + str(i) for i in range(hash_vector_size)])

# Display the hashed DataFrame
hashed_df.head()

cat_feats = X_train.select_dtypes(include=['object']).drop(columns=['Item_Identifier'])
ohe = OneHotEncoder (handle_unknown = 'ignore')
ohe.fit(cat_feats)
ohe_feature_names = ohe.get_feature_names_out(input_features=cat_feats.columns)

num_feats_train = X_train.select_dtypes(exclude=['object']).reset_index(drop=True)
cat_feats_train = X_train.select_dtypes(include=['object']).drop(columns=['Item_Identifier'])
X_train_cat_ohe = pd.DataFrame(ohe.transform(cat_feats_train).toarray(), columns=ohe_feature_names)
X_train_final = pd.concat([num_feats_train, hashed_df, X_train_cat_ohe], axis =1)
X_train_final

X_train_final.shape

gb = GradientBoostingRegressor(random_state=SEED)
train_and_eval_model(model=gb, X_train = X_train_final, y_train=y_train)

xgr = HistGradientBoostingRegressor(random_state=SEED)
train_and_eval_model(model=xgr, X_train = X_train_final, y_train=y_train)

X_test.shape

hashed_test_df = pd.DataFrame(fh.transform(X_test['Item_Identifier'].apply(lambda x: [x])).toarray(), # Changed this line
                         columns=['H' + str(i) for i in range(hash_vector_size)])
num_feats_test = X_test.select_dtypes(exclude=['object']).reset_index(drop=True)
cat_feats_test = X_test.select_dtypes(include=['object']).drop(columns=['Item_Identifier'])
X_test_cat_ohe = pd.DataFrame(ohe.transform(cat_feats_test).toarray(), columns=ohe_feature_names)
X_test_final = pd.concat([num_feats_test, hashed_test_df, X_test_cat_ohe], axis =1)
X_test_final.head()

X_test_final.shape

# Initialize GradientBoostingRegressor without the 'objective' argument
gb = GradientBoostingRegressor(random_state=SEED)

# Fit the model using the same training data
gb.fit(X_train_final, y_train)

y_pred = gb.predict(X_test_final)

from sklearn.metrics import r2_score, mean_squared_error

print ('R2 Score:', r2_score(y_test, y_pred))
print ('RMSE Score:', mean_squared_error(y_test, y_pred, squared = False))

import pandas as pd

# Get feature importance from the model
feature_importance = gb.feature_importances_

# Scale the feature importance to make them more readable
scaled_importance = feature_importance * 100

# Create a DataFrame with feature names and their importance
feature_importance_table = pd.DataFrame({
    'Feature': X_train_final.columns,
    'Importance (%)': scaled_importance
})

# Sort the DataFrame by importance in descending order
feature_importance_table = feature_importance_table.sort_values(by='Importance (%)', ascending=False)
# Set display options to show all rows
pd.set_option('display.max_rows', None)
# Display the table
print(feature_importance_table)

# Filter out features with zero importance
non_zero_importance_table = feature_importance_table[feature_importance_table['Importance (%)'] > 0]

# Display the table without zero-importance features
print(non_zero_importance_table)